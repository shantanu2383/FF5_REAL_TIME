# -*- coding: utf-8 -*-
"""FF5 Dataload.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j_rfJxBPAtlWCsldQHszg3My--dnekdu
"""



#Import Libraries
import pandas as pd
import statsmodels.formula.api as smf
import numpy as np
import matplotlib.pyplot as plt

!pip install pandasql
import os
from datetime import datetime
from sklearn.model_selection import train_test_split
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima_model import ARIMA
from sklearn.metrics import mean_squared_error, mean_absolute_error

import math
import matplotlib.pyplot as plt
from datetime import datetime
import seaborn as sns
import pandasql as ps
from sqlite3 import connect
from google.colab import drive
drive.mount("/content/gdrive", force_remount=True)

conn=connect(':memory:')

#SET PATHS
main="/content/gdrive/MyDrive/FIMA SUMMER 2023/RISK MANAGEMENT/FAMA FRENCH FACTORS/_main/"
raw= main + '_raw/'
aux= raw + 'Cake Shop Realtime Fundamentals as of June 23, 2023/'
clean= main + '_clean/'

"""# 01. Import Daily  Price Data"""

file="DailyStockPrice.csv"


price_df=pd.read_csv(raw + file)
# Parse the 'date' column to a datetime format
price_df['date'] = pd.to_datetime(price_df['date'], format='%Y-%m-%d')

# Filter to only keep data from 2018 onwards
price_df = price_df[(price_df['date'] >= "2015-01-01")]

# Parse Relevant Variables, only keep distinct rows
price_df = price_df[['ticker', 'date', 'adj_close', 'adj_volume']].drop_duplicates()
print(price_df)

"""# 02. Import Zachs Master Data as Ticker Map"""

file= "ZACKS_MT_2.csv"

# Read the CSV file into a DataFrame 'map'
map = pd.read_csv(aux + file)



# Keep distinct rows based on 'ticker', 'exchange', 'asset_type', 'comp_type'
map = map[['ticker', 'exchange', 'asset_type', 'comp_type']].drop_duplicates()

# Merge 'price_df' and 'map' DataFrames on the 'ticker' column
price_df_xmap = pd.merge(price_df, map, on='ticker', how='left')

# Convert to string type for filtering
price_df_xmap['exchange'] = price_df_xmap['exchange'].astype(str)
price_df_xmap['asset_type'] = price_df_xmap['asset_type'].astype(str)
price_df_xmap['comp_type'] = price_df_xmap['comp_type'].astype(str)

# Keep only rows where 'exchange' is either 'NYSE', 'AMEX', or 'NASDAQ'
relevant_exchanges = ['NYSE', 'AMEX', 'NASDAQ']
price_df_xmap = price_df_xmap[price_df_xmap['exchange'].isin(relevant_exchanges)]

# Keep only US Based common stocks where 'asset_type' is 'COM'
price_df_xmap = price_df_xmap[price_df_xmap['asset_type'] == 'COM']

# Keep only industrial stocks where 'comp_type' is '1.0'
price_df_xmap = price_df_xmap[price_df_xmap['comp_type'] == '1.0']

"""# 03. Calculate Daily Returns"""

price_df = price_df_xmap

# Sort by 'ticker' and 'date', so stocks are listed consecutively
price_df = price_df.sort_values(['ticker', 'date'], ascending=True)

# Create a 'stock checker' column, shifted by one for previous day's ticker
#price_df['stock checker'] = price_df['ticker'].shift(1)

# Only keep rows where 'ticker' matches 'stock checker', i.e., we have prior day data for the same stock
#price_df = price_df[price_df['ticker'] == price_df['stock checker']]

# Calculate the 'daily_return' column
price_df['adj_close'] = pd.to_numeric(price_df['adj_close'], errors='coerce')
price_df['adj_close_prev_day'] = price_df.groupby('ticker')['adj_close'].shift(1)
price_df['daily_return'] = ((price_df['adj_close'] - price_df['adj_close_prev_day']) / price_df['adj_close_prev_day']) * 100

# Parse relevant variables and remove duplicates
price_df = price_df[['ticker', 'date', 'adj_close', 'daily_return', 'exchange']].drop_duplicates()

price_df

"""# 04. Import Raw Fundamental Files"""

# Read data from CSV files
z_fc2 = pd.read_csv(aux + "ZACKS_FC_2.csv")
z_mktv2 = pd.read_csv(aux + "ZACKS_MKTV_2.csv")
z_fr2 = pd.read_csv(aux + "ZACKS_FR_2.csv")

# Keep distinct rows based on specified columns
z_mktv2 = z_mktv2.drop_duplicates()

z_fc2 = z_fc2.drop_duplicates()

z_fr2 = z_fr2.drop_duplicates()


# Only keep quarterly data
z_fr2 = z_fr2[z_fr2['per_type'] == "Q"]
z_fc2 = z_fc2[z_fc2['per_type'] == "Q"]
z_mktv2 = z_mktv2[z_mktv2['per_type'] == "Q"]

# Standardize 'per_end_date' and 'ticker' columns
for df in [z_fr2, z_fc2, z_mktv2]:
    df['per_end_date'] = pd.to_datetime(df['per_end_date'], format='%Y-%m-%d')
    df['ticker'] = df['ticker'].astype(str)

# Merge the dataframes on 'ticker' and 'per_end_date'
fundamentals = z_fc2.merge(z_fr2[['ticker', 'per_end_date', 'book_val_per_share']], how='left', on=['ticker', 'per_end_date'])
fundamentals = fundamentals.merge(z_mktv2[['ticker', 'per_end_date', 'mkt_val']], how='left', on=['ticker', 'per_end_date'])

# Keep only rows where 'exchange' is either "NYSE", "AMEX", or "NASDAQ"
fundamentals = fundamentals[fundamentals['exchange'].isin(["NYSE", "AMEX", "NASDAQ"])]

"""# ---------------------------------------------------"""

for i in fundamentals.columns:
  print(i)

'''Book Equity. Book equity is constructed from Compustat data or collected from the Moody’s Industrial,
Financial, and Utilities manuals. BE is the book value of stockholders’ equity, plus balance sheet deferred
taxes and investment tax credit (if available), minus the book value of preferred stock.
Depending on availability, we use the redemption, liquidation, or par value (in that order) to estimate the book value of preferred stock. Stockholders’ equity is the value reported by Moody’s or Compustat, if it is available. If not,
we measure stockholders’ equity as the book value of common equity plus the par value of preferred stock,
or the book value of assets minus total liabilities (in that order). '''

#set tax credits and preferred equity to 0 where missing

fundamentals['def_tax_asset_curr'] = fundamentals['def_tax_asset_curr'].fillna(0)
fundamentals['def_tax_asset_lterm'] = fundamentals['def_tax_asset_lterm'].fillna(0)
fundamentals['tot_pref_stock'] = fundamentals['tot_pref_stock'].fillna(0)

#Calculate book equity as per FF definition

fundamentals.to_sql('fundamentals', conn, if_exists='replace')

query='''
        SELECT DISTINCT *,
        COALESCE(tot_share_holder_equity, tot_asset-tot_liab) + def_tax_asset_curr + def_tax_asset_lterm - tot_pref_stock as book_value
        FROM fundamentals
        '''
fundamentals_book_value=pd.read_sql(query, conn)

fundamentals=fundamentals_book_value

fundamentals['book_value'].isnull().sum()

"""# 05. Pull Relevant Variables From Fundamentals

**Market Equity:**
"""

cleaned_returns = price_df.copy()

# Convert 'date' to datetime format and create a 'quarter' column
cleaned_returns['date'] = pd.to_datetime(cleaned_returns['date'])
cleaned_returns['quarter'] = cleaned_returns['date'].dt.to_period('Q')

# Create a 'last_quarter' column by subtracting one quarter from the 'date' column
cleaned_returns['last_quarter'] = (cleaned_returns['date'] - pd.DateOffset(months=3)).dt.to_period('Q')

# Convert 'per_end_date' to datetime format and create a 'quarter' column in fundamentals dataframe
fundamentals['per_end_date'] = pd.to_datetime(fundamentals['per_end_date'])
fundamentals['quarter'] = fundamentals['per_end_date'].dt.to_period('Q')

# Convert 'quarter' columns to string format for joining
cleaned_returns['last_quarter'] = cleaned_returns['last_quarter'].astype(str)
cleaned_returns['quarter'] = cleaned_returns['quarter'].astype(str)
fundamentals['quarter'] = fundamentals['quarter'].astype(str)

# Convert 'ticker' columns to string format
cleaned_returns['ticker'] = cleaned_returns['ticker'].astype(str)
fundamentals['ticker'] = fundamentals['ticker'].astype(str)

# Sort fundamentals dataframe by 'mkt_val'
fundamentals = fundamentals.sort_values(by='mkt_val', ascending=False)

# Merge cleaned_returns and fundamentals dataframes
cleaned_returns = cleaned_returns.merge(fundamentals[['ticker', 'quarter', 'mkt_val', 'comm_shares_out']], how='left',
                                        left_on=['ticker', 'last_quarter'],
                                        right_on=['ticker', 'quarter'], suffixes=('', '_y'))

# Rename 'mkt_val' to 'mkt_cap'
cleaned_returns.rename(columns={'mkt_val': 'mkt_cap_old'}, inplace=True)

# Remove unnecessary columns from the merged dataframe
cleaned_returns.drop(columns=['quarter_y'], inplace=True)

"""**Calculate ME as per FF: PRICE * SHARES OUTSTANDING**"""

cleaned_returns['mkt_cap']=cleaned_returns['comm_shares_out']*cleaned_returns['adj_close']

"""**Pull Book Value From The End of Each Year to Calculate B/M Ratio**"""

# To create a DataFrame similar to 'fundamentals_be' using the 'fundamentals' DataFrame:
fundamentals_be = fundamentals[['ticker', 'per_end_date', 'book_value']].copy()

# Convert 'per_end_date' to datetime format and extract 'quarter' and 'month'
fundamentals_be['per_end_date'] = pd.to_datetime(fundamentals_be['per_end_date'])
fundamentals_be['quarter'] = fundamentals_be['per_end_date'].dt.to_period('Q')
fundamentals_be['month'] = fundamentals_be['per_end_date'].dt.month

# Filter the DataFrame to keep only the entries where 'month' is 12
fundamentals_be = fundamentals_be[fundamentals_be['month'] == 12]

# Rename 'per_end_date' to 'date'
fundamentals_be.rename(columns={'per_end_date': 'date'}, inplace=True)
fundamentals_be

#add reference date for matching
fundamentals_be['year']=fundamentals_be['date'].dt.year

fundamentals_be['year']=pd.to_numeric(fundamentals_be['year'], errors='coerce')

#ADD REFERENCE DATE FOR MATCHING (6/1 of next calendar year)
fundamentals_be['reference_date']=fundamentals_be['year']+1
fundamentals_be['reference_date']=fundamentals_be['reference_date'].apply(str)
fundamentals_be['reference_date'] = fundamentals_be['reference_date'].apply(lambda x: x + "-06-01")
fundamentals_be['reference_date']=pd.to_datetime(fundamentals_be['reference_date'])
fundamentals_be.drop('year', inplace=True, axis=1)

fundamentals_be

"""Pull Total Asset Value From End of Year to Calculate Investment"""

fundamentals_inv=fundamentals[['ticker', 'per_end_date', 'tot_asset']].copy()

fundamentals_inv['per_end_date'] = pd.to_datetime(fundamentals_inv['per_end_date'])
fundamentals_inv['quarter'] = fundamentals_inv['per_end_date'].dt.to_period('Q')
fundamentals_inv['month'] = fundamentals_inv['per_end_date'].dt.month


fundamentals_inv = fundamentals_inv[fundamentals_inv['month'] == 12]

# Rename 'per_end_date' to 'date'
fundamentals_inv.rename(columns={'per_end_date': 'date'}, inplace=True)

fundamentals_inv.sort_values(by=['ticker', 'date'], inplace=True)
fundamentals_inv

"""Calculate investment as (assets t-1 -assets t-2)/[assets t-2]"""

#shift back investment by ticker
fundamentals_inv.sort_values(by=['ticker', 'date'], inplace=True)
fundamentals_inv['prev_year_tot_asset'] = fundamentals_inv.groupby('ticker')['tot_asset'].shift(1)

fundamentals_inv['inv']=(fundamentals_inv['tot_asset']-fundamentals_inv['prev_year_tot_asset'])/(fundamentals_inv['tot_asset'])

# Add reference date for matching
fundamentals_inv['year'] = fundamentals_inv['date'].dt.year

fundamentals_inv['year'] = pd.to_numeric(fundamentals_inv['year'], errors='coerce')

# Add reference date for matching (6/1 of next calendar year)
fundamentals_inv['reference_date'] = fundamentals_inv['year']+1
fundamentals_inv['reference_date'] = fundamentals_inv['reference_date'].apply(str)
fundamentals_inv['reference_date'] = fundamentals_inv['reference_date'].apply(lambda x: x + "-06-01")
fundamentals_inv['reference_date'] = pd.to_datetime(fundamentals_inv['reference_date'])
fundamentals_inv.drop('year', inplace=True, axis=1)

"""# 06a. Construct the Stocks Data"""

# Construct main_stocks_df
main_stocks_df = cleaned_returns[['ticker', 'exchange', 'date', 'daily_return', 'quarter', 'mkt_cap', 'adj_close', 'comm_shares_out']].copy()

# Rename column
main_stocks_df.rename(columns={'daily_return': 'ret'}, inplace=True)

# Convert 'date' to datetime
main_stocks_df['date'] = pd.to_datetime(main_stocks_df['date'])

# Add 'reference_date' column
main_stocks_df['reference_date'] = main_stocks_df['date'].apply(
    lambda dt: pd.Timestamp(year=dt.year - 1, month=6, day=1) if dt.month < 6 else pd.Timestamp(year=dt.year, month=6, day=1)
)

"""# 06b. Merge B/M Into Stocks Data"""

# Convert 'reference_date' columns to datetime
fundamentals_be['reference_date'] = pd.to_datetime(fundamentals_be['reference_date'])
main_stocks_df['reference_date'] = pd.to_datetime(main_stocks_df['reference_date'])

# Merge main_stocks_df and fundamentals_be dataframes on 'ticker' and 'reference_date'
main_stocks_df = main_stocks_df.merge(fundamentals_be[['ticker', 'reference_date', 'book_value']],
                                      on=['ticker', 'reference_date'],
                                      how='left')

"""# 06c. Add ME From End of Year"""

# Filter rows with 'date' in December
stocks_me = main_stocks_df[main_stocks_df['date'].dt.month == 12].copy()

# Select distinct rows based on 'ticker' and 'quarter'
stocks_me = stocks_me[['ticker', 'date', 'mkt_cap', 'quarter']].drop_duplicates()

# Convert the 'date' column to datetime format
stocks_me['date'] = pd.to_datetime(stocks_me['date'])

# Extract year and month
stocks_me['year'] = stocks_me['date'].dt.year
stocks_me['month'] = stocks_me['date'].dt.month

# Group by ticker, year, and month and select the last observation
stocks_me_last_obs = stocks_me.sort_values('date').groupby(['ticker', 'year', 'month']).last().reset_index()
stocks_me=stocks_me_last_obs


# Extract year and quarter from 'quarter' column
stocks_me[['year', 'quarter']] = stocks_me['quarter'].str.split('Q', expand=True)

# Convert 'year' to numeric
stocks_me['year'] = stocks_me['year'].astype(int)

# Create 'reference_date' for next year
stocks_me['reference_date'] = pd.to_datetime((stocks_me['year'] + 1).astype(str) + "-06-01")

# Rename 'mkt_cap' to 'mkt_equity'
stocks_me.rename(columns={'mkt_cap': 'mkt_equity'}, inplace=True)

# Drop unnecessary columns
stocks_me.drop(['year', 'quarter'], axis=1, inplace=True)

# Merge 'main_stocks_df' and 'stocks_me' on 'ticker' and 'reference_date'
main_stocks_df = pd.merge(main_stocks_df, stocks_me, on=['ticker', 'reference_date'], how='left')

"""#06d. Calculate BM Ratio: BE/ME"""

main_stocks_df['bm_ratio']=main_stocks_df['book_value']/main_stocks_df['mkt_equity']

negative_values = main_stocks_df[main_stocks_df['bm_ratio'] < 0]
count_negative = len(negative_values)
print("Number of negative values in 'bm_ratio':", count_negative)

"""# 06e. Merge Investments Into Stocks Data"""

# Convert 'reference_date' columns to datetime
fundamentals_inv['reference_date'] = pd.to_datetime(fundamentals_inv['reference_date'])
main_stocks_df['reference_date'] = pd.to_datetime(main_stocks_df['reference_date'])

# Merge main_stocks_df and fundamentals_be dataframes on 'ticker' and 'reference_date'
main_stocks_df = main_stocks_df.merge(fundamentals_inv[['ticker', 'reference_date', 'inv']],
                                      on=['ticker', 'reference_date'],
                                      how='left')

"""# 06f. Merge OP into Stocks Data"""

fundamentals_op=pd.read_csv(clean+"op.csv")

# Convert 'reference_date' columns to datetime
fundamentals_op['reference_date'] = pd.to_datetime(fundamentals_op['reference_date'])
main_stocks_df['reference_date'] = pd.to_datetime(main_stocks_df['reference_date'])

# Merge main_stocks_df and fundamentals_be dataframes on 'ticker' and 'reference_date'
main_stocks_df = main_stocks_df.merge(fundamentals_op[['ticker', 'reference_date', 'op']],
                                      on=['ticker', 'reference_date'],
                                      how='left')

main_stocks_df['op'].isnull().sum()

"""# 07. Size Sorts"""

main_stocks_df_copy=main_stocks_df.copy()



main_stocks_df.drop('date_y', axis=1, inplace=True)
main_stocks_df.rename(columns={'date_x': 'date'}, inplace=True)


# Pre-processing: replacing infinite values with NaN and dropping NaN values
main_stocks_df.replace([np.inf, -np.inf], np.nan, inplace=True)
main_stocks_df.dropna(inplace=True)

# Extract quarter from the date
main_stocks_df['date'] = pd.to_datetime(main_stocks_df['date'])
main_stocks_df['quarter'] = main_stocks_df['date'].dt.quarter

# Filter data for June and NYSE to calculate the median market cap
size_breakpoints = main_stocks_df[(main_stocks_df['date'].dt.month == 6) & (main_stocks_df['exchange'] == "NYSE")].copy()
size_breakpoints=size_breakpoints[['ticker', 'reference_date', 'mkt_cap', 'date']].drop_duplicates()

# Convert the 'date' column to datetime format
size_breakpoints['date'] = pd.to_datetime(size_breakpoints['date'])

# Group by ticker and reference_date and select the last observation
size_breakpoints_last_obs = size_breakpoints.sort_values('date').groupby(['ticker', 'reference_date']).last().reset_index()


size_breakpoints=size_breakpoints_last_obs
size_median = size_breakpoints.groupby('reference_date')['mkt_cap'].median().reset_index()
size_median.columns = ['reference_date', 'size_median']
size_median

size_breakpoints_last_obs

# Create new dataframe for all stocks in June
size_sorts = main_stocks_df[main_stocks_df['date'].dt.month == 6].copy()
size_sorts=size_sorts[['ticker', 'reference_date', 'mkt_cap', 'date']].drop_duplicates()

size_sorts_last_obs = size_sorts.sort_values('date').groupby(['ticker', 'reference_date']).last().reset_index()
size_sorts=size_sorts_last_obs

# Merge the median size into size_sorts
size_sorts = size_sorts.merge(size_median, on='reference_date', how='left')

#Assign to Relevant Size Portfolio
size_sorts['size_portfolio'] = np.where(size_sorts['mkt_cap'] > size_sorts['size_median'], 'B', 'S')

# Merge size_sorts data back into main_stocks_df
main_stocks_df = main_stocks_df.merge(size_sorts, on=['ticker', 'reference_date'], how='left')

main_stocks_df.drop(['date_y', 'mkt_cap_y'], axis=1, inplace=True)
main_stocks_df.rename(columns={'date_x':'date', 'mkt_cap_x':'mkt_cap'}, inplace=True)

"""# 08. Investment Sorts"""

main_stocks_df.replace([np.inf, -np.inf], np.nan, inplace=True)


# Filter data for June and NYSE
inv_breakpoints = main_stocks_df[(main_stocks_df['date'].dt.month == 6) & (main_stocks_df['exchange'] == "NYSE")].copy()

# Get distinct values of ticker, reference_date, and bm_ratio
inv_breakpoints = inv_breakpoints[['ticker', 'reference_date', 'inv']].drop_duplicates()

# Calculate the 30th and 70th quantiles of bm_ratio for each reference_date
inv_breakpoints['q30'] = inv_breakpoints.groupby('reference_date')['inv'].transform(lambda x: x.quantile(0.3))
inv_breakpoints['q70'] = inv_breakpoints.groupby('reference_date')['inv'].transform(lambda x: x.quantile(0.7))

inv_breakpoints = inv_breakpoints[['reference_date', 'q30', 'q70']].drop_duplicates()
inv_breakpoints

# Create new dataframe for all stocks in June
inv_sorts = main_stocks_df[main_stocks_df['date'].dt.month == 6].copy()
inv_sorts=inv_sorts[['ticker', 'reference_date', 'inv']].drop_duplicates()

#Merge in breakpoints
inv_sorts=inv_sorts.merge(inv_breakpoints, on='reference_date', how='left')

inv_breakpoints=inv_sorts

# Classify stocks as 'L', 'M', or 'H' based on the quantiles
inv_breakpoints['inv_portfolio'] = np.where(inv_breakpoints['inv'] > inv_breakpoints['q70'], 'H',
                                                np.where(inv_breakpoints['inv'] > inv_breakpoints['q30'], 'M', 'L'))


# Merge value_breakpoints data back into main_stocks_df
main_stocks_df = main_stocks_df.merge(inv_breakpoints[['ticker', 'reference_date', 'inv_portfolio']],
                                      on=['ticker', 'reference_date'], how='left')



main_stocks_df.to_csv()

"""# 08. Value Sorts"""

main_stocks_df_copy=main_stocks_df.copy()

# Prepare data- drop stocks with negative BE
main_stocks_df = main_stocks_df[main_stocks_df['bm_ratio'] >= 0]

main_stocks_df.replace([np.inf, -np.inf], np.nan, inplace=True)
main_stocks_df.dropna()

# Extract quarter from the date
main_stocks_df['date'] = pd.to_datetime(main_stocks_df['date'])
main_stocks_df['quarter'] = main_stocks_df['date'].dt.quarter

# Filter data for June and NYSE
value_breakpoints = main_stocks_df[(main_stocks_df['date'].dt.month == 6) & (main_stocks_df['exchange'] == "NYSE")].copy()

# Get distinct values of ticker, reference_date, and bm_ratio
value_breakpoints = value_breakpoints[['ticker', 'reference_date', 'bm_ratio']].drop_duplicates()

# Calculate the 30th and 70th quantiles of bm_ratio for each reference_date
value_breakpoints['q30'] = value_breakpoints.groupby('reference_date')['bm_ratio'].transform(lambda x: x.quantile(0.3))
value_breakpoints['q70'] = value_breakpoints.groupby('reference_date')['bm_ratio'].transform(lambda x: x.quantile(0.7))

value_breakpoints=value_breakpoints[['reference_date', 'q30', 'q70']].drop_duplicates()
value_breakpoints

# Create new dataframe for all stocks in June
value_sorts = main_stocks_df[main_stocks_df['date'].dt.month == 6].copy()
value_sorts=value_sorts[['ticker', 'reference_date', 'bm_ratio']].drop_duplicates()

#Merge in breakpoints
value_sorts=value_sorts.merge(value_breakpoints, on='reference_date', how='left')

value_breakpoints=value_sorts

# Classify stocks as 'L', 'M', or 'H' based on the quantiles
value_breakpoints['value_portfolio'] = np.where(value_breakpoints['bm_ratio'] > value_breakpoints['q70'], 'H',
                                                np.where(value_breakpoints['bm_ratio'] > value_breakpoints['q30'], 'M', 'L'))

# Merge value_breakpoints data back into main_stocks_df
main_stocks_df=main_stocks_df_copy
main_stocks_df = main_stocks_df.merge(value_breakpoints[['ticker', 'reference_date', 'value_portfolio']],
                                      on=['ticker', 'reference_date'], how='left')

negative_values = main_stocks_df[main_stocks_df['bm_ratio'] < 0]
count_negative = len(negative_values)
print("Number of negative values in 'bm_ratio':", count_negative)

"""# OP Sorts"""

op_breakpoints = main_stocks_df[(main_stocks_df['date'].dt.month == 6) & (main_stocks_df['exchange'] == "NYSE")].copy()
# Get distinct values of ticker, reference_date, and bm_ratio
op_breakpoints = op_breakpoints[['ticker', 'reference_date', 'op']].drop_duplicates()

# Calculate the 30th and 70th quantiles of bm_ratio for each reference_date
op_breakpoints['q30'] = op_breakpoints.groupby('reference_date')['op'].transform(lambda x: x.quantile(0.3))
op_breakpoints['q70'] = op_breakpoints.groupby('reference_date')['op'].transform(lambda x: x.quantile(0.7))

op_breakpoints = op_breakpoints[['reference_date', 'q30', 'q70']].drop_duplicates()
op_breakpoints

# Create new dataframe for all stocks in June
op_sorts = main_stocks_df[main_stocks_df['date'].dt.month == 6].copy()
op_sorts=op_sorts[['ticker', 'reference_date', 'op']].drop_duplicates()

op_sorts=op_sorts.merge(op_breakpoints, on='reference_date', how='left')

op_breakpoints=op_sorts

# Classify stocks as 'L', 'M', or 'H' based on the quantiles
op_breakpoints['op_portfolio'] = np.where(op_breakpoints['op'] > op_breakpoints['q70'], 'H',
                                                np.where(op_breakpoints['op'] > op_breakpoints['q30'], 'M', 'L'))

# Merge op_breakpoints data back into main_stocks_df

main_stocks_df = main_stocks_df.merge(op_breakpoints[['ticker', 'reference_date', 'op_portfolio']],
                                      on=['ticker', 'reference_date'], how='left')

"""# Export Data"""

clean= main + '_clean/'
main_stocks_df.to_csv(clean + "FF5 Dataload 07_23.csv")

len(main_stocks_df)

main

f=main_stocks_df[main_stocks_df['inv'].isnull()]